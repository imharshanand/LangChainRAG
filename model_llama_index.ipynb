{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a22633-e324-4a79-891b-55fd369d5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/rag/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries from transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Specify the path to the local GPT-2 model\n",
    "model_name = \"./gpt2-large\"\n",
    "\n",
    "# Load the tokenizer and model from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text-generation pipeline using the loaded model and tokenizer\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Display a message confirming the pipeline setup\n",
    "print(\"Pipeline setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a2b6b1-de76-4ed1-bca6-0497c9a4a1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted PDF Text Preview:\n",
      " Personal Information\n",
      "Name: John Michael Smith\n",
      "Date of Birth: January 15, 1985\n",
      "Place of Birth: New York City, USA\n",
      "Nationality: American\n",
      "Gender: Male\n",
      "Marital Status: Married\n",
      "Contact Information:\n",
      "- Address: 1234 Elm Street, Apt. 5B, Brooklyn, NY, 11215\n",
      "- Phone: (123) 456-7890\n",
      "- Email: john.m.smith@example.com\n",
      "Personal Statement\n",
      "John Michael Smith is a highly motivated and results-driven professional\n",
      "with over 15 years of experience in the software development industry.\n",
      "Known for his exceptional pro\n"
     ]
    }
   ],
   "source": [
    "# Import PyMuPDF for PDF processing\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Define a function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF file\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF document\n",
    "pdf_path = \"demo_data_for_RAG.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display the first 500 characters of the extracted text to verify\n",
    "print(\"Extracted PDF Text Preview:\\n\", pdf_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7347a89-dba7-44ba-a695-8233b1820c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and populated with document embeddings using SentenceTransformer.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a list of documents including the text from the PDF\n",
    "documents = [\n",
    "    pdf_text,\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial Intelligence is transforming the world.\",\n",
    "    \"LangChain provides a powerful interface for working with language models.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for each document using the SentenceTransformer model\n",
    "document_embeddings = embedding_model.encode(documents, convert_to_tensor=True).cpu().detach().numpy()\n",
    "\n",
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])  # Initialize a flat (non-hierarchical) index\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Display a message confirming the index setup\n",
    "print(\"FAISS index created and populated with document embeddings using SentenceTransformer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c987544-4928-4b07-bdff-48a118b9626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context:\n",
      " Artificial Intelligence is transforming the world.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to retrieve the most relevant context based on a question\n",
    "def retrieve_context(question):\n",
    "    # Generate an embedding for the query using the SentenceTransformer model\n",
    "    query_embedding = embedding_model.encode([question], convert_to_tensor=True).cpu().detach().numpy()\n",
    "    \n",
    "    # Search the FAISS index to find the most similar document embedding\n",
    "    distances, indices = index.search(query_embedding, 1)\n",
    "    \n",
    "    # Return the most relevant document (context)\n",
    "    return documents[indices[0][0]]\n",
    "\n",
    "# Example usage of the retrieval function\n",
    "context_example = retrieve_context(\"Tell me about AI.\")\n",
    "print(\"Retrieved Context:\\n\", context_example[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49368be7-c8e9-4afd-ae8b-50272fcbb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom LLMChain initialized.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary classes from langchain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template for generating text based on the retrieved context\n",
    "template = \"\"\"\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a concise answer based on the context above.\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "# Initialize the prompt template\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "# Define a custom LLMChain class to integrate retrieval and generation\n",
    "class CustomLLMChain:\n",
    "    def __init__(self, pipeline, prompt):\n",
    "        self.pipeline = pipeline\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Retrieve context using the question\n",
    "        context = retrieve_context(inputs[\"question\"])\n",
    "        \n",
    "        # Format the prompt with the retrieved context and question\n",
    "        prompt_input = {\"context\": context, \"question\": inputs[\"question\"]}\n",
    "        prompt_text = self.prompt.format(**prompt_input)\n",
    "        \n",
    "        # Generate the response using the pipeline with max_new_tokens\n",
    "        output = self.pipeline(prompt_text, max_new_tokens=50)[0]['generated_text']\n",
    "        return output\n",
    "\n",
    "# Initialize the custom LLMChain\n",
    "llm_chain = CustomLLMChain(pipeline=pipe, prompt=prompt)\n",
    "\n",
    "# Display a message confirming the LLMChain setup\n",
    "print(\"Custom LLMChain initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee7ee63-e874-49e1-a561-f04079a8fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      " \n",
      "Context: Personal Information\n",
      "Name: John Michael Smith\n",
      "Date of Birth: January 15, 1985\n",
      "Place of Birth: New York City, USA\n",
      "Nationality: American\n",
      "Gender: Male\n",
      "Marital Status: Married\n",
      "Contact Information:\n",
      "- Address: 1234 Elm Street, Apt. 5B, Brooklyn, NY, 11215\n",
      "- Phone: (123) 456-7890\n",
      "- Email: john.m.smith@example.com\n",
      "Personal Statement\n",
      "John Michael Smith is a highly motivated and results-driven professional\n",
      "with over 15 years of experience in the software development industry.\n",
      "Known for his exceptional problem-solving skills and ability to lead teams to\n",
      "success, John is dedicated to advancing technology and improving user\n",
      "experiences.\n",
      "Key Strengths\n",
      "Strong leadership and team management skills\n",
      "Expertise in software development and project management\n",
      "Excellent communication and interpersonal abilities\n",
      "Proficient in multiple programming languages and technologies\n",
      "Adaptable and quick to learn new skills\n",
      "Education and Work Experience\n",
      "Education\n",
      "Master of Science in Computer Science\n",
      "New York University, New York, NY\n",
      "Graduated: May 2010\n",
      "- GPA: 3.8/4.0\n",
      "- Thesis: “Advancements in Machine Learning Algorithms for Predictive\n",
      "Analytics”\n",
      "Bachelor of Science in Computer Engineering\n",
      "University of California, Berkeley, CA\n",
      "Graduated: May 2007\n",
      "- GPA: 3.7/4.0\n",
      "- Capstone Project: “Developing a Real-Time Traffic Management System”\n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "Work Experience\n",
      "Senior Software Engineer\n",
      "Tech Innovations Inc., New York, NY\n",
      "June 2015 - Present\n",
      "- Led a team of 10 developers in designing and implementing a scalable\n",
      "cloud-based application - Improved system performance by 30% through\n",
      "code optimization and refactoring - Managed project timelines, ensuring\n",
      "timely delivery and adherence to budget\n",
      "Software Engineer\n",
      "Creative Solutions LLC, San Francisco, CA\n",
      "July 2010 - May 2015\n",
      "- Developed and maintained web applications for clients across various\n",
      "industries - Collaborated with cross-functional teams to gather requirements\n",
      "and deliver custom solutions - Conducted code reviews and provided\n",
      "mentorship to junior developers\n",
      "Intern\n",
      "Innovative Tech Corp., Berkeley, CA\n",
      "June 2006 - August 2006\n",
      "- Assisted in the development of a mobile application for real-time data\n",
      "analysis - Performed testing and debugging to ensure software quality -\n",
      "Documented code and created user manuals for the application\n",
      "Skills, Certifications, and Personal Interests\n",
      "Skills\n",
      "Programming Languages: Java, Python, C++, JavaScript, SQL\n",
      "Web Technologies: HTML, CSS, React, Angular, Node.js\n",
      "Databases: MySQL, PostgreSQL, MongoDB\n",
      "Tools & Platforms: Git, Docker, Kubernetes, AWS, Azure\n",
      "Methodologies: Agile, Scrum, DevOps\n",
      "Certifications\n",
      "Certified Scrum Master (CSM), Scrum Alliance, 2016\n",
      "AWS Certified Solutions Architect – Associate, Amazon Web\n",
      "Services, 2018\n",
      "Certified Kubernetes Administrator (CKA), Cloud Native\n",
      "Computing Foundation, 2019\n",
      "Personal Interests\n",
      "Travel: Passionate about exploring new cultures and countries; visited\n",
      "over 20 countries\n",
      "Photography: Enjoy capturing landscapes and urban life; won local\n",
      "photography contests\n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "Sports: Avid runner and marathon participant; also enjoys playing\n",
      "basketball and soccer\n",
      "Reading: Enjoys reading science fiction and technology-related books\n",
      "• \n",
      "• \n",
      "\n",
      "\n",
      "Question: What is the full name of John?\n",
      "\n",
      "Please provide a concise answer based on the context above.\n",
      "Answer: The full name is John Michael Smith (John M. Smith)\n",
      "\n",
      "Context: Personal Information\n",
      "\n",
      "Name: John Michael Smith\n",
      "\n",
      "Date of Birth: Jan 15, 1985\n",
      "\n",
      "Place of Birth: New York City, USA\n",
      "\n",
      "National\n"
     ]
    }
   ],
   "source": [
    "# Define a sample query\n",
    "question = \"What is the full name of John?\"\n",
    "\n",
    "# Use the LLMChain to generate an answer based on retrieved context\n",
    "result = llm_chain({\"question\": question})\n",
    "\n",
    "# Print the generated answer\n",
    "print(\"Generated Answer:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05be447-54a1-4e39-800d-99560779e9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
